{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedarredondo/data-science-fundamentals/blob/main/Unit3/Unit3NotesSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"preliz[full,notebook]\""
      ],
      "metadata": {
        "id": "3xvGOrVlmKf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57t4h39aopvE"
      },
      "outputs": [],
      "source": [
        "import preliz as pz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import pymc as pm"
      ],
      "metadata": {
        "id": "p0bxCpvRoy4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unit 3: Making and Modifying Models\n",
        "\n",
        "We will:\n",
        "\n",
        "- Practice Selecting Priors\n",
        "- Practice Selecting Likelihoods, using posterior predictive checks\n",
        "- Begin to focus on matching reality, instead of matching data\n"
      ],
      "metadata": {
        "id": "XhNHQtb3pv19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Selecting Priors\n",
        "\n",
        "There are three types of priors:\n",
        "- Informative priors, defined as priors that \"strongly\" direct our model. These are often devised using our knowldge about the context in question.\n",
        "- Uninformative priors, defined as priors that make the least amount of assuptions as possible. [Uninformative means different things in different contexts.](https://stats.stackexchange.com/questions/20520/what-is-an-uninformative-prior-can-we-ever-have-one-with-truly-no-information?newreg=acc7fa13b7cf4e72bcdbeaec65d6b586)\n",
        "- Nonsensical priors, AKA \"bad\" priors. These are priors that in some way contradict what's known about the context or contradict our likelihood.\n",
        "\n",
        "Let's start with some sensical priors, both informative and uninformative."
      ],
      "metadata": {
        "id": "67LopBrPklZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resonable priors, and their differences\n",
        "\n",
        "Recall the Victor Wembanyama FT example from last unit. I chose that example because there's a very inuitive choice for a prior, specifically Wembanyama's FT shooting in Euroleague play.\n",
        "\n",
        "However, we didn't have to base our prior on his prior FT shooting. We could have arbitraily chosen a distribution that \"felt right\" to us. As long as we don't contradict reality or our likelihood, things will be find.\n",
        "\n"
      ],
      "metadata": {
        "id": "kzMJt-ie-SdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do some examples:"
      ],
      "metadata": {
        "id": "Zek_E2YBERxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wemby's FTs from his first 2 NBA games\n",
        "FT_data = np.repeat([1,0], repeats = [7,2])"
      ],
      "metadata": {
        "id": "zGQQbmnERlOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same model from before, but named wmbyE_FT_model, where to E is for Euroleauge\n",
        "\n",
        "with pm.Model() as wmbyE_FT_model:\n",
        "  #we define our prior with the makes and misses from Euroleague\n",
        "  θ = pm.Beta('θ' , alpha = 24. , beta = 8.)\n",
        "\n",
        "  #The Bernoulli likelihood is a stand in for the binomial distribution\n",
        "  y = pm.Bernoulli('y', p = θ, observed = FT_data)\n",
        "\n",
        "  #sample from our posterior\n",
        "  idata_FT_E = pm.sample(2000)"
      ],
      "metadata": {
        "id": "ugzE_T6NdJWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualize the 2000 samples from the pymc model of the posterior\n",
        "az.plot_posterior(idata_FT_E)"
      ],
      "metadata": {
        "id": "moaOq_GQa0Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**:\n",
        "\n",
        "Make a new model wmbyU_FT_model, where the prior is a uniform distribution, but everything is otherwise the same."
      ],
      "metadata": {
        "id": "lDwPAXH6GLYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ANSWER1\n"
      ],
      "metadata": {
        "id": "LPsBxwkLGr0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANSWER1 cont.\n"
      ],
      "metadata": {
        "id": "N1hSk4UKH19y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task2**:\n",
        "\n",
        "Ask least one other person about possible similarities and differences between the posterior's of wmbyE_FT_model and wmbyU_FT_model. Then, write down what you both said."
      ],
      "metadata": {
        "id": "JbO2q7e6V5KP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer2**:\n"
      ],
      "metadata": {
        "id": "wGFFQ9k9X8E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**:\n",
        "\n",
        "Describe the differences in how we'd interpret wmbyE_FT_model versus wmbyU_FT_model. How would using one instead of the other change our predictions?"
      ],
      "metadata": {
        "id": "JTdcRub6iG88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer3**:\n"
      ],
      "metadata": {
        "id": "qfoq-PTSireA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nonsensical Priors\n",
        "\n",
        "We've now seen that our choice of prior can meaningfully change our model and predictions, at least for this small data set of 9 FT attempts.\n",
        "\n",
        "And that was with priors that made sense. Let's see what happens when we use a prior that doesn't make sense."
      ],
      "metadata": {
        "id": "xMXNr77wYtRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to do several things wrong.\n",
        "\n",
        "- My prior will be centered around 0.1 .\n",
        "- I'll use a normal dsitribution.\n",
        "- 78% of the probability of my prior will be between 0 and 0.2 .\n",
        "\n",
        "Here's what the prior looks like:"
      ],
      "metadata": {
        "id": "gcBTx0Qs07s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finds the maximum entropy distrbution with the restriction that 0.78 of its\n",
        "#mass is between 0 and 0.2\n",
        "\n",
        "pz.maxent(pz.Normal(), lower = 0, upper = 0.2, mass = 0.78)"
      ],
      "metadata": {
        "id": "j4FAPi2IsQfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task4**:\n",
        "\n",
        "All the assumptions I made in making the prior are \"bad\", in this context.\n",
        "\n",
        "Explain why."
      ],
      "metadata": {
        "id": "-aNHgS6C4QMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer4**:\n",
        "\n"
      ],
      "metadata": {
        "id": "NPN_2wJF4qD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know why a $Normal(μ=0.1,σ=0.08)$ prior is unreasonable, let's see what happens when we use it."
      ],
      "metadata": {
        "id": "aVm6tXKS6y-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same model from before, but named wmbyN_FT_model,\n",
        "#where to N is for normal\n",
        "\n",
        "with pm.Model() as wmbyN_FT_model:\n",
        "  #our nonsensical prior\n",
        "  θ = pm.Normal('θ', mu=0.1,sigma=0.08)\n",
        "\n",
        "  y = pm.Bernoulli('y', p = θ, observed = FT_data)\n",
        "\n",
        "  idata_FT_N = pm.sample(2000)"
      ],
      "metadata": {
        "id": "UyBZHFYdvSOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pymc has given us an indication that we did something wrong. At the right most end of output text it says \"divergences\".\n",
        "\n",
        "Remember how I told you to think of the engine that powers are model like a car? If our model has more than 0 divergences, consider one of the warning lights on.\n",
        "\n",
        "Divergences can mean anything from \"proceed with caution\" to \"throw your model in the trash an burn it\". In this case, we don't need to be experts in Markov Chain managment to diagnose what's wrong: its that 11% chance of a negative FT% that's causing all the confusion."
      ],
      "metadata": {
        "id": "tFU2KPB38D9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite being suspect, our model still ran, and it produced the following posterior:"
      ],
      "metadata": {
        "id": "EuPeHiqCCDIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_posterior(idata_FT_N)"
      ],
      "metadata": {
        "id": "3A3G-dVyvoF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task5**:\n",
        "\n",
        "Is there anything about the posterior distribution that tells you that the model is poor enough that we shouldn't trust its predictions?"
      ],
      "metadata": {
        "id": "mF7vc-xECHlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer5**:\n",
        "\n"
      ],
      "metadata": {
        "id": "S2JbxgMGCmXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task6**:\n",
        "\n",
        "What happens if we made our prior even more nonsensical? What would be an example of that?"
      ],
      "metadata": {
        "id": "-8LQTFImEdvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer6**:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVqLz6wuEoev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dominates Priors\n",
        "\n",
        "While nonsensical priors can ruin a model, as long as we state a prior that only gives values in our event space--from 0 to 1 in the FT example--then things will work out.\n",
        "\n",
        "To see why, let's rerun all three models: the informative beta prior model, the uninformative uniform prior model, and the nonsensical normal prior model."
      ],
      "metadata": {
        "id": "YtVX1ckDEDgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wemby's FTs in the 2023/2024 season, makes then misses\n",
        "FT_data_2324 = np.repeat([1,0], repeats = [292,75])"
      ],
      "metadata": {
        "id": "1Ce9fbfr2PYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2023/2024 wmbyE_FT_model, where to E is for Euroleauge\n",
        "\n",
        "with pm.Model() as wmbyE_FT_model:\n",
        "  #we define our prior with the makes and misses from Euroleague\n",
        "  θ = pm.Beta('θ' , alpha = 24. , beta = 8.)\n",
        "\n",
        "  #The Bernoulli likelihood is a stand in for the binomial distribution\n",
        "  y = pm.Bernoulli('y', p = θ, observed = FT_data_2324)\n",
        "\n",
        "  #sample from our posterior\n",
        "  idata_FT_E2324 = pm.sample(2000)"
      ],
      "metadata": {
        "id": "6tHNXYgB2e6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_posterior(idata_FT_E2324)"
      ],
      "metadata": {
        "id": "oIF0Ifgs3c3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2023/2024 wmbyU_FT_model, where to U is for Uniform\n",
        "\n",
        "with pm.Model() as wmbyU_FT_model:\n",
        "  #we define our prior with the makes and misses from Euroleague\n",
        "  θ = pm.Uniform('θ' , lower = 0 , upper = 1)\n",
        "\n",
        "  #The Bernoulli likelihood is a stand in for the binomial distribution\n",
        "  y = pm.Bernoulli('y', p = θ, observed = FT_data_2324)\n",
        "\n",
        "  #sample from our posterior\n",
        "  idata_FT_U2324 = pm.sample(2000)"
      ],
      "metadata": {
        "id": "VbvyNaFm2gno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_posterior(idata_FT_U2324)"
      ],
      "metadata": {
        "id": "ekdnj6by3iAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task7**:\n",
        "\n",
        "Describe similarities and differences between the posteriors of wmbyE_FT_model and wmbyU_FT_model. Give an explanation for these similarities and differences."
      ],
      "metadata": {
        "id": "tt4U3Riu4xuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer7**:\n",
        "\n"
      ],
      "metadata": {
        "id": "ErlXHhg55NfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2023/2024 wmbyN_FT_model, where to N is for Normal\n",
        "\n",
        "with pm.Model() as wmbyN_FT_model:\n",
        "  #we define our prior with the makes and misses from Euroleague\n",
        "  θ = pm.Normal('θ' , mu = 0.1 , sigma = 0.08)\n",
        "\n",
        "  #The Bernoulli likelihood is a stand in for the binomial distribution\n",
        "  y = pm.Bernoulli('y', p = θ, observed = FT_data_2324)\n",
        "\n",
        "  #sample from our posterior\n",
        "  idata_FT_N2324 = pm.sample(2000)"
      ],
      "metadata": {
        "id": "hAC7fNEA29u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_posterior(idata_FT_N2324)"
      ],
      "metadata": {
        "id": "MWJ7GLzx3jvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task8**:\n",
        "\n",
        "Describe similarities and differences between the posterior of wmbyN_FT_model and the other two posteriors, wmbyE_FT_model and wmbyU_FT_model. Give an explanation for these similarities and differences."
      ],
      "metadata": {
        "id": "zdJ5Hyxo7yB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer8**:\n",
        "\n"
      ],
      "metadata": {
        "id": "TWStGOTm8BnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may make it seem like priors don't matter for large data sets, and that's somewhat true; for simple models, uninformative, or lightly informative priors are all we'll need. The rest is usually overkill.\n",
        "\n",
        "But for the complex models found later in the course, priors will make a big comeback utility.\n",
        "\n",
        "For now though, let's switch to the other big choice we make in desgining a model."
      ],
      "metadata": {
        "id": "6Y8d9VLb-UH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Likelihoods\n",
        "\n",
        "So if priors aren't the most powerful part of our model, what is?\n",
        "\n",
        "The likelihood!\n",
        "\n"
      ],
      "metadata": {
        "id": "Ai563_t2Cjxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the FT. We didn't directly conceptualize Wemby's FTs as a binomial distribution. We ended up modeling each free throw using a bernoulli distribution, which lacks information about the number of attempts. This works out because the number of attempts is included in the data. You saw this in the output of the our predictions in Unit 2.\n",
        "\n",
        "Our choice of likelhood depended on the structure of our data, our context, and the question we're trying to answer.\n",
        "\n",
        "Let's see this in action with a new context, and new set of data."
      ],
      "metadata": {
        "id": "-bZg8rr_vZqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to download the data to this colab notebook. I'll also take a moment to introducce the data set.\n",
        "\n",
        "Nuclear magnetic resonance (NMR) produces values known as chemical shifts, which is defined as the difference between the resonant frequency of spinning protons and a reference molecule. These chemical shifts are used to identify properties of various compounds, often in organic chemistry and related fields.\n",
        "\n",
        "We'll use this data for three reasons:\n",
        "- To show you that the techniques in this class apply to all things, from NBA FT shooting to quantum chemistry applications.\n",
        "- Its a great illustrative example for both likelihoods and heirarchical models (next unit) that our textbook has already set up.\n",
        "- Its cool as heck."
      ],
      "metadata": {
        "id": "_HyCz4JY5gik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data from my github page. Works as long as you have an internet\n",
        "#which you should if you're using colab.\n",
        "url_chemshift = 'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/chemical_shifts.csv'\n",
        "#I could have put the url string directly into pd.read_csv()\n",
        "data_chemshift = pd.read_csv(url_chemshift)"
      ],
      "metadata": {
        "id": "S-GulKKa5v_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view raw data\n",
        "#I did this, colab offered to plot it for me, which is wonderful\n",
        "#I'd already made a histogram (shown in the next code block), but its a rad feature.\n",
        "#There appear to be other features--use them, they seem neat!\n",
        "data_chemshift"
      ],
      "metadata": {
        "id": "3OvcUYhhF908"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I visualize the raw data as a histogram with 10 bins (the default)\n",
        "plt.hist(data_chemshift)"
      ],
      "metadata": {
        "id": "ppiHn46H_-TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task9**:\n",
        "\n",
        "Describe the histogram. Be as detailed as possible."
      ],
      "metadata": {
        "id": "7DeR5kGqHmF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer9**:\n"
      ],
      "metadata": {
        "id": "HvIxhF-dH84d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to make a model that captures pattern(s) in our data. How many of the patterns? We'll spend the rest of the course disscussing how to capture more patterns, and whether or not we should.\n",
        "\n",
        "For right now, let's assume we want to capture all the patterns, but we don't know how."
      ],
      "metadata": {
        "id": "cSNOD04OIqou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our raw data and histogram, its clear that we should't use a bernoulli likelihood; our data isn't limited to just 0 or 1. In fact, its not even discrete, as these chemical shift values look like that can take on any positive real number."
      ],
      "metadata": {
        "id": "PD5_mhjANI6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've just seen a distribution that covers all real numbers--the Normal distribution!\n",
        "\n",
        "The normal distribution is a good pick as a likelihood if two things are true:\n",
        "- our data apears to come from the real numbers\n",
        "- the only context we have is that our data varies symmetrically around a value.\n",
        "\n",
        "The first contraint is true, but the second is a little dubious. Let's run a model with this likelihood assumption anyway, and then make it better later."
      ],
      "metadata": {
        "id": "-U2_mlE9Nxb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why just shove a normal distribution in to get the model running? Because we still need to think about our priors. That's right, two priors: one for the expected value our data varies around, and a second one for how much the data varies around that mean.\n",
        "\n",
        "The following are graphs of the priors I'll use."
      ],
      "metadata": {
        "id": "WUVmnoYcSYMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unifrom prior, because I don't understand this context at all\n",
        "# 45 is a little lower than the smallest chemical shift,\n",
        "# 70 is a little higher than the largest chemical shift.\n",
        "pz.Uniform(45,70).plot_pdf()"
      ],
      "metadata": {
        "id": "daanrY6dTcqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll use a new distribution for the prior of my standard deviation.\n",
        "\n",
        "Why half a normal distribution? Because the half normal:\n",
        "- only contains the positive reals, and our standard deviation (the parameter for variation) from the center is always positive.\n",
        "- it has most of its weight towards small values of the reals; this prevents our model from having to test massive values for our variation."
      ],
      "metadata": {
        "id": "GYp0bmzaUTU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# why sigma = 5? B/c our textbook used it, and it seems to work well\n",
        "pz.HalfNormal(sigma = 5).plot_pdf()"
      ],
      "metadata": {
        "id": "MGdtfBdZTn8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the full model."
      ],
      "metadata": {
        "id": "bC5sXBD6ULgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#N for normal\n",
        "with pm.Model() as model_chemshiftN:\n",
        "    # I have no idea where the mean should be, but its reasonalbe to assume\n",
        "    # its between our lowest and highest values recorded\n",
        "    μ = pm.Uniform('μ', lower=45, upper=70)\n",
        "\n",
        "    #this one is new, but its exactly what it sounds like: half of a normal dist\n",
        "    σ = pm.HalfNormal('σ', sigma=5)\n",
        "\n",
        "    #our fancy new likelihood, with two priors\n",
        "    Y = pm.Normal('Y', mu = μ, sigma = σ, observed=data_chemshift)\n",
        "    idata_chemshiftN = pm.sample()"
      ],
      "metadata": {
        "id": "vlmDmixIH_kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_posterior(idata_chemshiftN)"
      ],
      "metadata": {
        "id": "Mq6YqGKdJ8iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our posterior is actually  two dimensional now, since we have two priors. If you want to see which pairs of $μ$ and $σ$ our model recommends, you can use the code below."
      ],
      "metadata": {
        "id": "uCy8fateV0iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code block is taken directly from the textbook BAP3.\n",
        "\n",
        "az.plot_pair(idata_chemshiftN, kind='kde', marginals=True)"
      ],
      "metadata": {
        "id": "QLJr6cL0WLVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual graph of the posterior will take up less of our attention as we move through this course; we often only care about the posterior because we use it to make predictions.\n",
        "\n",
        "To that end, we'll use posterior predictive checks (ppc)."
      ],
      "metadata": {
        "id": "RaQNVl4WWSyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_chemshiftN, model=model_chemshiftN, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "z6DXWqhaXA6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code block is taken directly from the textbook BAP3.\n",
        "#Plots the posterior predictive density overlaid with a smoothed version of the data\n",
        "\n",
        "az.plot_ppc(idata_chemshiftN, num_pp_samples=100, figsize=(12, 4), colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "7xmkClPgXOhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task10**:\n",
        "\n",
        "Using the graph above, answer the following questions:\n",
        "\n",
        "1. Does model_chemshiftN model the chemshift data well?\n",
        "2. How confident should we be in those predictions?"
      ],
      "metadata": {
        "id": "ozSijRNVXm6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer10**:\n"
      ],
      "metadata": {
        "id": "OSSOqNYSY8Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Normality\n",
        "\n",
        "One way to make our predictions better is to change how we handle outliers--those two values that are away from the rest of the distribution. We can do this by picking a dist that's \"flatter\"--one that expects to see more outliers.\n",
        "\n",
        "Comments on notation: $ν$ is pronouced \"nu\". You may also call $ν$ our normaility parameter.  A higher $ν$ means more normal."
      ],
      "metadata": {
        "id": "lzeHB0UdbAmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code block is taken from the textbook BAP3, with comments added.\n",
        "#Focus only on the output.\n",
        "\n",
        "#plot four T dists, with four different normality parameters\n",
        "for nu in [1, 2, 10, 30]:\n",
        "    #all have mean 0 and standard deviation 1\n",
        "    pz.StudentT(nu, 0, 1).plot_pdf(support=(-5, 5), figsize=(12, 4))\n",
        "\n",
        "#T dist with normality parameter of infinity\n",
        "ax = pz.StudentT(np.inf, 0, 1).plot_pdf(support=(-5, 5), figsize=(12, 4), color=\"k\")\n",
        "\n",
        "#make one of the dist have a dashed lin\n",
        "ax.get_lines()[-1].set_linestyle(\"--\")\n",
        "#provides legend to read the graph better.\\\n",
        "pz.internal.plot_helper.side_legend(ax)"
      ],
      "metadata": {
        "id": "dhprkKpDi4JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task11**:\n",
        "\n",
        "For what value of $ν$ does the T dist become the normal dist?"
      ],
      "metadata": {
        "id": "ilVZckUak-7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer11**:\n",
        "\n"
      ],
      "metadata": {
        "id": "HDpWUuQulOyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a prior for $ν$, and I'll choose the following."
      ],
      "metadata": {
        "id": "xjdb-mP4mn98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pz.Exponential(lam = 1/30).plot_pdf()"
      ],
      "metadata": {
        "id": "t8qUI1rsiAtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task12**:\n",
        "\n",
        "Why is this a good choice of prior for $ν$?"
      ],
      "metadata": {
        "id": "9-FbMH87m39A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer12**:\n",
        "\n"
      ],
      "metadata": {
        "id": "QNuW_QRgm_BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you understand $ν$ and its prior, run the next model with the Student T likelihood."
      ],
      "metadata": {
        "id": "t7wGPBalsXXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#T is for Student's T distribution\n",
        "with pm.Model() as model_chemshiftT:\n",
        "    # weak prior\n",
        "    μ = pm.Uniform('μ', lower=45, upper=70)\n",
        "\n",
        "    #the same as before\n",
        "    σ = pm.HalfNormal('σ', sigma=5)\n",
        "\n",
        "    #this is our normality parameter; lam = 1/30 gives a mean of 30\n",
        "    ν = pm.Exponential('ν', lam = 1/30)\n",
        "\n",
        "    #new likelihood, same data\n",
        "    y = pm.StudentT('y', mu=μ, sigma=σ, nu = ν, observed=data_chemshift)\n",
        "    idata_chemshiftT = pm.sample()"
      ],
      "metadata": {
        "id": "ktNuZXCFeSlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_chemshiftT, model=model_chemshiftT, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "CNZMceSIeIij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code block is taken from the textbook BAP3.\n",
        "#Plots the posterior predictive density overlaid with a smoothed version of the data\n",
        "\n",
        "ax = az.plot_ppc(idata_chemshiftT, num_pp_samples=100, figsize=(12, 4), colors=[\"C1\", \"C0\", \"C1\"])\n",
        "#need to limit the x axis, so its more readable\n",
        "ax.set_xlim(40, 70)"
      ],
      "metadata": {
        "id": "aZR7kw_OSa-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task13**:\n",
        "\n",
        "Using the graph above, answer the following questions:\n",
        "\n",
        "1. Does model_chemshiftT model the chemshift data well?\n",
        "2. How confident should we be in those predictions?"
      ],
      "metadata": {
        "id": "3a6_4v13ovpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer13**:\n",
        "\n"
      ],
      "metadata": {
        "id": "LtYvY-99qf9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Matching Reality, Not the Data\n",
        "\n",
        "We don't always want our model to perfectly match the data--we want our model to match reality.\n",
        "\n",
        "I'll show you one more model with the chemshift data to get you thinking about the assumptions we make when we select a likelihood.\n"
      ],
      "metadata": {
        "id": "rxRWXT6YxCyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task14**:\n",
        "\n",
        "What assumption did we make about the outliers when we selected the T distribution as our likelihood in model-chemshiftT?"
      ],
      "metadata": {
        "id": "-WFYflr9eC8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer14**:\n",
        "\n"
      ],
      "metadata": {
        "id": "DqNpmHw9f1Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we think that there is a consistent low chance of observing values much higher than average, then we think our distribution has skew. We can model this with a SkewNormal disttribution; see the following example."
      ],
      "metadata": {
        "id": "vSncGHgMhNMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SN is for skew normal\n",
        "with pm.Model() as model_chemshiftSN:\n",
        "    #same weak prior\n",
        "    μ = pm.Uniform('μ', lower=45, upper=70)\n",
        "\n",
        "    #same as before\n",
        "    σ = pm.HalfNormal('σ', sigma=5)\n",
        "\n",
        "    #This gives our model the ability to skew towards either the positive numbers\n",
        "    #or towards the negative numbers\n",
        "    α = pm.Exponential('α', lam = 1/2)\n",
        "\n",
        "    #SkewNormal, which is the same as a normal, except with more unusually high\n",
        "    #or unusually low values.\n",
        "    y = pm.SkewNormal('y', mu=μ, sigma=σ, alpha = α, observed=data_chemshift)\n",
        "    idata_chemshiftSN = pm.sample()"
      ],
      "metadata": {
        "id": "lDIHVKOLyP7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_chemshiftSN, model=model_chemshiftSN, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "RFAtkYq405tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_ppc(idata_chemshiftSN, num_pp_samples=100, figsize=(12, 4), colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "oKUw8rIpTbgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task15**:\n",
        "\n",
        "Using the graph above, answer the following questions:\n",
        "\n",
        "1. Does model_chemshiftSN model the chemshift data well? Better than model_chemshiftT?\n",
        "2. How confident should we be in those predictions?\n"
      ],
      "metadata": {
        "id": "0_0OiFuThxE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer15**:\n",
        "\n"
      ],
      "metadata": {
        "id": "Ku536lumiBnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary\n",
        "\n",
        "In this unit we covered how to select priors and likelihods, and started to consider how those choices affect our models and predictions.\n",
        "\n",
        "Priors are a big deal for small data sets, and in some worst case scenarios, can cripple our model. But with even a moderate amount of data, they serve mainly to keep track of our assumptions (for now . . . ). We are free to choose whatever prior we want, as long as we are willing to accept the consequences.\n",
        "\n",
        "Likelihoods are always a big deal. We learned that a great starting point is the normal distribution. Normality means we only know or care about center and variation; this assumption can be relaxed with a StudentT distribution. If we additionally care about skew, we can use a skew normal.\n",
        "\n",
        "Fianlly, the point of all these choices is not neccesarily to get a model that lines up neatly with our data, but to get a model that captures the important patterns in our data, and ONLY those important patterns.\n",
        "\n",
        "Your exercises this unit will force you to explore the different distributions you can use for priors and likelihoods, and your project will call on you to apply those distributions to a model making context of your choice."
      ],
      "metadata": {
        "id": "G98uAM7ktqS8"
      }
    }
  ]
}